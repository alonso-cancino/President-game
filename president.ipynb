{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d9614473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alonso\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from gym import spaces\n",
    "import torch\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "from collections import deque,namedtuple\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "def encode(lst):\n",
    "    deck = []\n",
    "    for pinta in ['O','B','C','E']:\n",
    "        for n in range(10):\n",
    "            deck.append(str(n)+pinta)\n",
    "    encoded = [0]*40\n",
    "    for card_lst in lst:\n",
    "        for card in card_lst:\n",
    "            encoded[deck.index(card)]=1\n",
    "    return encoded\n",
    "\n",
    "class president_game:\n",
    "    def __init__(self,n_j):\n",
    "        self.players_data = []\n",
    "        self.stack = []\n",
    "        self.history = []\n",
    "        self.n_j = n_j\n",
    "        self.deck = []\n",
    "        self.active_player = None\n",
    "        self.last_player = None\n",
    "        for pinta in ['O','B','C','E']:\n",
    "            for n in range(10):\n",
    "                self.deck.append(str(n)+pinta)\n",
    "    \n",
    "    def deal(self):\n",
    "        self.history = []\n",
    "        self.players_data = []\n",
    "        self.stack = []\n",
    "        available_indices = list(range(40))\n",
    "        for player in range(self.n_j):\n",
    "            sample = random.sample(available_indices,int(40/self.n_j))\n",
    "            self.players_data.append([self.deck[ind] for ind in sample])\n",
    "            available_indices = list(set(available_indices) - set(sample))\n",
    "            \n",
    "    def active_player_0(self):\n",
    "        if self.active_player == None:\n",
    "            for player in range(self.n_j):\n",
    "                if '0O' in self.players_data[player]:\n",
    "                    self.active_player = player\n",
    "        \n",
    "    def possible_plays(self, player = None):\n",
    "        if player == None:\n",
    "            active_hand = self.players_data[self.active_player]\n",
    "        else:\n",
    "            active_hand = self.players_data[player]\n",
    "        curr_quantity = len(self.stack)\n",
    "        if curr_quantity == 0:\n",
    "            if ('0O' in active_hand):\n",
    "                pos_plays = [lst for lst in list(powerset([c for c in active_hand if c[0]=='0'])) if ('0O' in lst)]\n",
    "                #for p in range(len(pos_plays)):\n",
    "                    #print(str(p)+': ',pos_plays[p])\n",
    "                return pos_plays          \n",
    "        elif self.stack == ['*']:\n",
    "            pos_plays = [lst for lst in list(powerset(active_hand)) if len(list(set([c[0] for c in lst])))==1]\n",
    "            pos_plays.append([])\n",
    "            return pos_plays\n",
    "        else:\n",
    "            curr_number = int(self.stack[0][0])\n",
    "            pos_plays = [lst for lst in itertools.combinations(active_hand, curr_quantity) if (len(list(set([c[0] for c in lst])))==1) and (int(lst[0][0]) > curr_number)]\n",
    "            #print([lst for lst in itertools.combinations(active_hand, curr_quantity) if (len(list(set([c[0] for c in lst])))==1)])\n",
    "            pos_plays.append([])\n",
    "            return pos_plays\n",
    "     \n",
    "    def play_select(self, selection = None,render = False):\n",
    "        if self.last_player == self.active_player:\n",
    "            self.stack = ['*']\n",
    "        pos_plays = self.possible_plays()\n",
    "        if render == True:\n",
    "            for ind in range(len(pos_plays)):\n",
    "                print(ind,': ',pos_plays[ind])\n",
    "        if selection == None:\n",
    "            selection = int(input('Que jugada elige :'))\n",
    "        if pos_plays[selection] != []:\n",
    "            self.stack = list(pos_plays[selection])\n",
    "            self.history.append(list(pos_plays[selection]))\n",
    "            self.last_player = self.active_player\n",
    "            self.players_data[self.active_player] = list(set(self.players_data[self.active_player])-set(pos_plays[selection]))\n",
    "        self.active_player = (self.active_player + 1)% self.n_j\n",
    "        return self\n",
    "        \n",
    "    def hand_eval(self, player):\n",
    "        hand_evaluation = 0\n",
    "        for c in self.players_data[player]:\n",
    "            hand_evaluation += -int(c[0])-1\n",
    "        return hand_evaluation\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d286c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class president_gym(gym.Env):\n",
    "    def __init__(self,n_players):\n",
    "        super().__init__()\n",
    "        self.n_players = n_players\n",
    "        self.observation_space = spaces.MultiDiscrete([2]*80+[4,10])\n",
    "        self.action_space = spaces.Discrete(41)\n",
    "        self.placements = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game = president_game(self.n_players)\n",
    "        self.game.deal()\n",
    "        self.game.active_player_0()\n",
    "        self.history = []\n",
    "        act = self.game.active_player\n",
    "        act_hand = self.game.players_data[act]\n",
    "        self.state = encode([act_hand,self.game.history])\n",
    "        return self \n",
    "        \n",
    "    def step(self,action,reward='placement'):\n",
    "        still_playing = [player for player in range(self.n_players) if len(self.game.players_data[player])!=0] \n",
    "        if len(still_playing)<=1:\n",
    "            if len(still_playing)==1:\n",
    "                self.placement.append(still_playing[0])\n",
    "            done = True\n",
    "        self.game.play_select(action)\n",
    "        self.state = encode([act_hand,history])\n",
    "        info = ''\n",
    "        \n",
    "        if reward == 'placement':\n",
    "            if done == True:\n",
    "                reward = -100 * (placement.index(self.game.active_player))\n",
    "            else:\n",
    "                reward = 0\n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "    def render(self, godmode=False):\n",
    "        print(f'Active player is player {self.game.active_player} with hand {self.game.players_data[self.game.active_player]}')\n",
    "        print(f'With possible plays :{self.game.possible_plays()}')\n",
    "        print(f'Game history so far is: {self.game.history}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5ca71d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jugador 3 tiene mano ['5B', '0E', '7O', '6O', '0O', '1B', '2E', '3E', '5O', '3B']\n",
      "[('0O',), ('0E', '0O')]\n",
      "[1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "president_train = president_gym(4)\n",
    "president_seed = president_gym(4)\n",
    "president_train.reset()\n",
    "act = president_train.game.active_player\n",
    "act_hand = president_train.game.players_data[act]\n",
    "print(f'Jugador {act} tiene mano {act_hand}')\n",
    "state = [act_hand,president_train.history]\n",
    "encoded_state = encode(state)\n",
    "print(president_train.game.possible_plays())\n",
    "print(encoded_state)\n",
    "pos_plays = president_train.game.possible_plays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "15dbe038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alonso\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "plays = [[0,0]]\n",
    "for number in range(0,10):\n",
    "    for amount in range(1,5):\n",
    "        plays.append([amount,number])\n",
    "\n",
    "def encode_play(lst):\n",
    "    pos_plays = []\n",
    "    for play in lst:\n",
    "        if play == []:\n",
    "            pos_plays.append([0,0])\n",
    "        elif play != []:\n",
    "            if len(set([p[0] for p in play])) == 1:\n",
    "                pos_plays.append([len(play),int(play[0][0])])\n",
    "    return pos_plays\n",
    "\n",
    "def masked_weights(pos_plays,weights,plays=plays):\n",
    "    return [weights[ind]*(plays[ind] in pos_plays)-100000*(plays[ind] not in pos_plays) for ind in range(len(plays))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e9545656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,state_space_dim,action_space_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(state_space_dim,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64*2,action_space_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.to(device)\n",
    "        return self.linear(x)\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.memory = deque(maxlen = capacity)\n",
    "    def push(self,state,action,next_state,reward):\n",
    "        self.memory.append((state,action,next_state,reward))\n",
    "    def sample(self,batch_size):\n",
    "        batch_size = min(batch_size,len(self))\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "\n",
    "def choose_action_epsilon_greedy(net,state,epsilon):\n",
    "    \n",
    "    if epsilon > 1 or epsilon < 0:\n",
    "        raise Exception('The epsilon value must be between 0 and 1.')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state,dtype=torch.float32)\n",
    "        net_out = net(state)\n",
    "    \n",
    "    best_action = int(net_out.argmax())\n",
    "    \n",
    "    action_space_dim = net_out.shape[-1]\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "        non_optimal_actions = [a for a in range(action_space_dim) if a!= best_action]\n",
    "        action = random.choice(non_optimal_actions)\n",
    "    else:\n",
    "        action = best_action\n",
    "        \n",
    "    return action, net_out.cpu().numpy()\n",
    "\n",
    "def choose_action_softmax(net, state, temperature):\n",
    "    \n",
    "    if temperature < 0:\n",
    "        raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "        \n",
    "    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "    if temperature == 0:\n",
    "        return choose_action_epsilon_greedy(net, state, 0)\n",
    "    \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Apply softmax with temp\n",
    "    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "    softmax_out = nn.functional.softmax(net_out/temperature, dim=0).cpu().numpy()\n",
    "                \n",
    "    # Sample the action using softmax output as mass pdf\n",
    "    all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "    # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
    "    action = np.random.choice(all_possible_actions,p=softmax_out)\n",
    "    \n",
    "    return action, net_out.cpu().numpy()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3c2c23db",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-184-b7ffcde1276e>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-184-b7ffcde1276e>\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    output_dim =\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "\n",
    "N_PLAYERS = 4\n",
    "NUM_GAMES = 10\n",
    "\n",
    "game = president_gym(4)\n",
    "game = game.reset()\n",
    "game.render()\n",
    "\n",
    "active_player = game.game.active_player\n",
    "hand = game.game.players_data[active_player]\n",
    "pos_plays = game.game.possible_plays()\n",
    "stack = game.game.stack\n",
    "history = game.game.history\n",
    "\n",
    "print(pos_plays)\n",
    "\n",
    "net_input_stack = encode_play([stack])\n",
    "net_input_hand = encode([hand])\n",
    "net_input_history = encode([history])\n",
    "net_input_pos_plays = encode_play(pos_plays)\n",
    "#print(net_input_hand + net_input_history + net_input_stack)\n",
    "\n",
    "net_input = torch.tensor(net_input_hand + net_input_history + net_input_stack[0], dtype = torch.float32)\n",
    "input_dim = net_input.shape[-1]\n",
    "output_dim = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "36634f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible plays : [[1, 0], [2, 0], [2, 0], [3, 0]]\n",
      "Masked neural net output : [-100000.0, -0.014269459992647171, -0.02080719918012619, 0.1937359869480133, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[3, 0]\n",
      "3\n",
      "history: [['0E', '0C', '0O']]\n",
      "Possible plays : [[0, 0]]\n",
      "Masked neural net output : [0.11270283907651901, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "0\n",
      "history: [['0E', '0C', '0O']]\n",
      "Possible plays : [[0, 0]]\n",
      "Masked neural net output : [0.06311618536710739, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "0\n",
      "history: [['0E', '0C', '0O']]\n",
      "Possible plays : [[0, 0]]\n",
      "Masked neural net output : [0.17636948823928833, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "0\n",
      "history: [['0E', '0C', '0O']]\n",
      "Possible plays : [[0, 0]]\n",
      "Masked neural net output : [0.09803179651498795, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "0\n",
      "history: [['0E', '0C', '0O'], ['9E']]\n",
      "Possible plays : [[0, 0]]\n",
      "Masked neural net output : [0.2554539442062378, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "0\n",
      "history: [['0E', '0C', '0O'], ['9E']]\n",
      "Possible plays : [[0, 0]]\n",
      "Masked neural net output : [0.22291314601898193, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "0\n",
      "history: [['0E', '0C', '0O'], ['9E']]\n",
      "Possible plays : [[0, 0]]\n",
      "Masked neural net output : [0.41507232189178467, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "0\n",
      "history: [['0E', '0C', '0O'], ['9E']]\n",
      "Possible plays : [[0, 0]]\n",
      "Masked neural net output : [0.335249125957489, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "0\n",
      "history: [['0E', '0C', '0O'], ['9E'], ['6E']]\n",
      "Possible plays : [[1, 7], [1, 8], [0, 0]]\n",
      "Masked neural net output : [0.1958538293838501, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -0.16594401001930237, -100000.0, -100000.0, -100000.0, 0.03424903750419617, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "2\n",
      "history: [['0E', '0C', '0O'], ['9E'], ['6E']]\n",
      "Possible plays : [[1, 7], [1, 8], [1, 9], [1, 8], [0, 0]]\n",
      "Masked neural net output : [0.16925892233848572, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -100000.0, -0.15316149592399597, -100000.0, -100000.0, -100000.0, 0.05302204191684723, -100000.0, -100000.0, -100000.0, -0.13522788882255554, -100000.0, -100000.0, -100000.0]\n",
      "[0, 0]\n",
      "4\n",
      "history: [['0E', '0C', '0O'], ['9E'], ['6E']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "import copy\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "plays = [[0,0]]\n",
    "for number in range(0,10):\n",
    "    for amount in range(1,5):\n",
    "        plays.append([amount,number])\n",
    "\n",
    "N_PLAYERS = 4\n",
    "NUM_GAMES = 1\n",
    "MAX_NUMBER_GAME_STEPS = 10\n",
    "PLAY_TYPE = 'self-play'\n",
    "TARGET_NET_UPDATE_STEPS = 10\n",
    "REPLAY_MEMORY_CAPACITY = 10000\n",
    "LR = 1e-3\n",
    "game = president_gym(N_PLAYERS)\n",
    "replay_mem = ReplayMemory(REPLAY_MEMORY_CAPACITY)\n",
    "\n",
    "policy_net = DQN(82+N_PLAYERS-1, 41).to(device)\n",
    "\n",
    "target_net = DQN(82+N_PLAYERS-1, 41).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(),lr = LR)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "for game_index in range(NUM_GAMES):\n",
    "    \n",
    "    game = game.reset()\n",
    "    game_done = False\n",
    "    game_steps = 0\n",
    "    \n",
    "    while not game_done and game_steps <= MAX_NUMBER_GAME_STEPS:\n",
    "        \n",
    "        still_playing = [hand for hand in game.game.players_data if len(hand)!=0]\n",
    "        \n",
    "        if len(still_playing)<=1:\n",
    "            \n",
    "            placements += [game.game.players_data.index(still_playing[0])]\n",
    "            game_done = True\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            active_player = game.game.active_player\n",
    "            hand = game.game.players_data[active_player]\n",
    "            other_hand_lengths = [len(game.game.players_data[ind]) for ind in range(N_PLAYERS) if ind != active_player]\n",
    "            pos_plays = game.game.possible_plays()\n",
    "            encoded_pos_plays = encode_play(pos_plays)\n",
    "            stack = game.game.stack\n",
    "            history = game.game.history\n",
    "\n",
    "            net_input_stack = encode_play([stack])\n",
    "            net_input_hand = encode([hand])\n",
    "            net_input_history = encode(history)\n",
    "            net_input_pos_plays = encode_play(pos_plays)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                net_input = torch.tensor(net_input_hand + net_input_history + other_hand_lengths + net_input_stack[0],dtype = torch.float32)\n",
    "                net_output = policy_net(net_input)\n",
    "                masked_output = masked_weights(encoded_pos_plays,net_output.detach().cpu().numpy())\n",
    "                print('Possible plays :',encoded_pos_plays)\n",
    "                print('Masked neural net output :',masked_output)\n",
    "                print(plays[np.argmax(masked_output)])\n",
    "                #print(best_pos_play)\n",
    "                #print(net_output)\n",
    "                print(encoded_pos_plays.index(plays[np.argmax(masked_output)]))\n",
    "                game.game.play_select(encoded_pos_plays.index(plays[np.argmax(masked_output)]))\n",
    "                print(f'history: {game.game.history}')\n",
    "        game_steps +=1\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f496947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06916992 -0.00265957 -0.4783727   0.03750735 -0.24600804  0.10422891\n",
      "  0.0759403  -0.15358374  0.08696938 -0.11341097  0.2536936   0.01497917\n",
      " -0.5329273   0.29117742 -0.2256351  -0.29870304 -0.10671773  0.26369077\n",
      "  0.246369   -0.28758222 -0.25909916  0.26477763  0.11355833  0.02271875\n",
      " -0.4679014  -0.05509595  0.04152231  0.30295223 -0.0774397   0.01080476\n",
      "  0.06304856  0.05994857  0.00648457 -0.03037679 -0.08925813  0.03488246\n",
      " -0.04815163 -0.11345432  0.01472715  0.0635439  -0.26832205] 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alonso\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "NUM_GAMES = 100\n",
    "\n",
    "MAX_GAME_STEPS = 200\n",
    "\n",
    "Q_net = DQN(2,41)\n",
    "\n",
    "for game_index in range(NUM_GAMES):\n",
    "    pass\n",
    "\n",
    "state = [1,2]\n",
    "\n",
    "net_input = torch.tensor(state,dtype=torch.float32)\n",
    "net_output = Q_net.linear.forward(net_input).detach().numpy()\n",
    "\n",
    "print(net_output,np.argmax(net_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "53162764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alonso\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define exploration profile\n",
    "initial_value = 5\n",
    "num_iterations = 800\n",
    "exp_decay = np.exp(-np.log(initial_value) / num_iterations * 6) \n",
    "exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]\n",
    "\n",
    "\n",
    "# environment\n",
    "env = gym.make('Acrobot-v1') \n",
    "env.seed(0) # Set a random seed for the environment \n",
    "\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"STATE SPACE SIZE: {state_space_dim}\")\n",
    "print(f\"ACTION SPACE SIZE: {action_space_dim}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "gamma = 0.99  \n",
    "replay_memory_capacity = 10000   \n",
    "lr = 1e-3\n",
    "target_net_update_steps = 10   \n",
    "batch_size = 256   \n",
    "bad_state_penalty = 0   \n",
    "min_samples_for_training = 1000   \n",
    "\n",
    "# replay memory\n",
    "replay_mem = ReplayMemory(replay_memory_capacity)    \n",
    "\n",
    "# policy network\n",
    "policy_net = DQN(state_space_dim, action_space_dim).to(device)\n",
    "\n",
    "# target network with the same weights of the policy network\n",
    "target_net = DQN(state_space_dim, action_space_dim).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e2d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
